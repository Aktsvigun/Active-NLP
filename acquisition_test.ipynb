{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "from neural_ner.util import Trainer, Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parse():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'conll'\n",
    "        self.result_path = 'neural_ner/results'\n",
    "        self.usemodel = 'CNN_BiLSTM_CRF'\n",
    "        self.worddim = 100\n",
    "        self.pretrnd = 'wordvectors/glove.6B.100d.txt'\n",
    "        self.reload = 0\n",
    "        self.num_epochs = 10\n",
    "\n",
    "opt=Parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "parameters = OrderedDict()\n",
    "import os\n",
    "\n",
    "\n",
    "parameters['model'] = opt.usemodel\n",
    "parameters['wrdim'] = opt.worddim\n",
    "parameters['ptrnd'] = opt.pretrnd\n",
    "\n",
    "if opt.usemodel == 'CNN_BiLSTM_CRF':\n",
    "    parameters['lower'] = 1\n",
    "    parameters['zeros'] = 0\n",
    "    parameters['cpdim'] = 0\n",
    "    parameters['dpout'] = 0.5\n",
    "    parameters['chdim'] = 25\n",
    "    parameters['tgsch'] = 'iobes'\n",
    "\n",
    "    parameters['wldim'] = 200\n",
    "    parameters['cldim'] = 25\n",
    "    parameters['cnchl'] = 25\n",
    "    \n",
    "    parameters['lrate'] = 0.015\n",
    "    parameters['acqmd'] = 'd'\n",
    "    \n",
    "elif opt.usemodel == 'CNN_BiLSTM_CRF_MC':\n",
    "    parameters['lower'] = 1\n",
    "    parameters['zeros'] = 0\n",
    "    parameters['cpdim'] = 0\n",
    "    parameters['dpout'] = 0.5\n",
    "    parameters['chdim'] = 25\n",
    "    parameters['tgsch'] = 'iobes'\n",
    "\n",
    "    parameters['wldim'] = 200\n",
    "    parameters['cldim'] = 25\n",
    "    parameters['cnchl'] = 25\n",
    "    \n",
    "    parameters['lrate'] = 0.015\n",
    "    parameters['acqmd'] = 'm'\n",
    "\n",
    "elif opt.usemodel == 'CNN_CNN_LSTM':\n",
    "    parameters['lower'] = 1\n",
    "    parameters['zeros'] = 0\n",
    "    parameters['cpdim'] = 0\n",
    "    parameters['dpout'] = 0.5\n",
    "    parameters['chdim'] = 25\n",
    "    parameters['tgsch'] = 'iobes'\n",
    "    \n",
    "    parameters['w1chl'] = 800\n",
    "    parameters['w2chl'] = 800\n",
    "    parameters['cldim'] = 25\n",
    "    parameters['cnchl'] = 50\n",
    "    parameters['dchid'] = 20\n",
    "    \n",
    "    parameters['lrate'] = 0.001\n",
    "    parameters['acqmd'] = 'd'\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "dataset_path = os.path.join('datasets',opt.dataset)\n",
    "result_path = os.path.join(opt.result_path, opt.dataset)\n",
    "model_name = opt.usemodel\n",
    "model_load = opt.reload\n",
    "init_percent = 2\n",
    "acquire_percent = 2\n",
    "acquire_method = 'random'\n",
    "loader = Loader()\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "    \n",
    "if not os.path.exists(os.path.join(result_path, model_name)):\n",
    "    os.makedirs(os.path.join(result_path, model_name))\n",
    "\n",
    "if not os.path.exists(os.path.join(result_path, model_name, 'active_checkpoints', acquire_method)):\n",
    "    os.makedirs(os.path.join(result_path, model_name, 'active_checkpoints', acquire_method))\n",
    "\n",
    "if opt.dataset == 'conll':\n",
    "    train_data, dev_data, test_data, test_train_data, mappings = loader.load_conll(dataset_path, parameters)\n",
    "    \n",
    "word_to_id = mappings['word_to_id']\n",
    "tag_to_id = mappings['tag_to_id']\n",
    "char_to_id = mappings['char_to_id']\n",
    "word_embeds = mappings['word_embeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model_path = '/home/ubuntu/Active-NLP/neural_ner/results/conll/CNN_BiLSTM_CRF_MC/active_checkpoint/mnlp/00004078/modelweights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index=set()\n",
    "\n",
    "def get_mnlp_mc(dataset, model_path, decoder, num_tokens, nsamp=100):\n",
    "    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    model.train(True)\n",
    "    tm = time.time()\n",
    "    \n",
    "    def get_metrics(j, data):\n",
    "        if j not in train_index:\n",
    "            sentence = data['words']\n",
    "            tags = data['tags']\n",
    "            chars = data['chars']\n",
    "            caps = data['caps']\n",
    "            if decoder=='CRF':\n",
    "                decoded = [model.decode(sentence, tags, chars, caps, usecuda=False) for itr in range(nsamp)]\n",
    "            elif decoder=='LSTM':\n",
    "                raise NotImplementedError()\n",
    "            score = np.array([itm[0].data[0] for itm in decoded])\n",
    "            tag_seq_list = [str(itm[1]) for itm in decoded]\n",
    "            tprobs = score/len(sentence)\n",
    "            tvarsc = Counter(tag_seq_list).most_common(1)[0][1]\n",
    "        else:\n",
    "            tprobs = np.ones(nsamp)*float('Inf')\n",
    "            tvarsc = float('Inf')\n",
    "        \n",
    "        return tprobs, tvarsc\n",
    "    \n",
    "    loopoutput = [get_metrics(j, data) for j, data in enumerate(dataset[:100])]\n",
    "    probs = np.array([litm[0] for litm in loopoutput])\n",
    "    varsc = np.array([litm[1] for litm in loopoutput])\n",
    "    \n",
    "    '''\n",
    "    probsmean = np.mean(probs, axis=1)\n",
    "    test_indices = np.argsort(varsc)\n",
    "    #test_indices = np.lexsort((varsc, probsmean))\n",
    "    cur_tokens=0\n",
    "    cur_indices = set()\n",
    "    i = 0\n",
    "    while cur_tokens<num_tokens:\n",
    "        cur_indices.add(test_indices[i])\n",
    "        cur_tokens += len(dataset[test_indices[i]]['words'])\n",
    "        i+=1\n",
    "    train_index.update(cur_indices)\n",
    "    '''\n",
    "\n",
    "    print ('Acquisition took %d seconds.' %(time.time()-tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mnlp_mc(train_data, model_path, decoder='CRF', num_tokens=4000, nsamp=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index=set()\n",
    "\n",
    "model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "model.train(True)\n",
    "\n",
    "def unwrap_decode(sentence, tags, chars, caps, usecuda=False):\n",
    "    return model.decode(sentence, tags, chars, caps, usecuda=False)\n",
    "\n",
    "def get_mnlp_mc(dataset, model_path, decoder, num_tokens, nsamp=100):\n",
    "    \n",
    "    tm = time.time()\n",
    "    probs = np.ones((len(dataset),nsamp))*float('Inf')\n",
    "    varsc = np.ones(len(dataset))*float('Inf')\n",
    "    for j, data in enumerate(dataset[:10]):\n",
    "        if j not in train_index:\n",
    "            sentence = data['words']\n",
    "            tags = data['tags']\n",
    "            chars = data['chars']\n",
    "            caps = data['caps']\n",
    "            if decoder=='CRF':\n",
    "                decoded = Parallel(n_jobs= -1, backend=\"threading\")\\\n",
    "                                      (delayed(unwrap_decode)(sentence, tags, chars, \n",
    "                                       caps, usecuda=False) for itr in range(nsamp))\n",
    "            elif decoder=='LSTM':\n",
    "                raise NotImplementedError()\n",
    "            score = np.array([itm[0].data[0] for itm in decoded])\n",
    "            tag_seq_list = [str(itm[1]) for itm in decoded]\n",
    "            probs[j,:] = score/len(sentence)\n",
    "            varsc[j] = Counter(tag_seq_list).most_common(1)[0][1]\n",
    "    \n",
    "    probsmean = np.mean(probs, axis=1)\n",
    "    test_indices = np.argsort(varsc)\n",
    "    #test_indices = np.lexsort((varsc, probsmean))\n",
    "    cur_tokens=0\n",
    "    cur_indices = set()\n",
    "    i = 0\n",
    "    while cur_tokens<num_tokens:\n",
    "        cur_indices.add(test_indices[i])\n",
    "        cur_tokens += len(dataset[test_indices[i]]['words'])\n",
    "        i+=1\n",
    "    train_index.update(cur_indices)\n",
    "\n",
    "    print ('Acquisition took %d seconds.' %(time.time()-tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[np.zeros(5) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p27]",
   "language": "python",
   "name": "conda-env-pytorch_p27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
